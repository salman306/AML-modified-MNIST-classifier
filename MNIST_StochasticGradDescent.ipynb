{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__All the functions needed to run the neural net__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## activation function\n",
    "def act_func(a, switch_ = 2):\n",
    "    if switch_ == 1: # linear activation\n",
    "        g = a\n",
    "    elif switch_ == 2: # sigmoid activation\n",
    "        g = 1.0 / (1.0 + np.exp(-1.0 * a))\n",
    "    elif switch_ == 3: # tanh() activation\n",
    "        g = np.tanh(a)\n",
    "    return g\n",
    "\n",
    "## neural net itself\n",
    "def neur_net(x, w_list, b, num_lay):\n",
    "    h_list = []\n",
    "    # set input as the first layer ([1, N])\n",
    "    h = x\n",
    "    h_list.append(h)\n",
    "    #### for each layer ([1, N+H])\n",
    "    for lay in range(num_lay-1):\n",
    "        # neuron pre-activation\n",
    "        a = b[lay] + np.dot(w_list[lay].T, h)\n",
    "\n",
    "        # activation function for layer 1\n",
    "        h = act_func(a)\n",
    "        h_list.append(h)\n",
    "\n",
    "    #pre-activation for output layer  (N+H+1)\n",
    "    lay = num_lay-1\n",
    "    a_out = b[lay] + np.dot(w_list[lay].T, h)\n",
    "\n",
    "    # activation function for output layer\n",
    "    f = act_func(a_out)\n",
    "    h_list.append(f)\n",
    "    return f, h_list\n",
    "\n",
    "# initialize weights in the correct shape\n",
    "def init_weights(num_neur_per_lay):\n",
    "    w_list = []\n",
    "#     np.random.seed(1)\n",
    "    for i in range(len(num_neur_per_lay)-1):\n",
    "    # initialize the weights so that they're in the right format\n",
    "        w = 2 * np.random.random((num_neur_per_lay[i], num_neur_per_lay[i+1])) - 1\n",
    "        w_list.append(w)\n",
    "#     print(w_list)\n",
    "    return w_list\n",
    "\n",
    "# activation function\n",
    "def der_act_func(x, switch_ = 2):\n",
    "    if switch_ == 1: # linear activation\n",
    "        g = 1\n",
    "    elif switch_ == 2: # sigmoid activation\n",
    "        g =  x * (1.0-x)\n",
    "    #### TO-DO\n",
    "#     elif switch_ == 3: # tanh() activation\n",
    "#         g = np.tanh(a)\n",
    "    return g\n",
    "\n",
    "# update the weights\n",
    "def update_w(error, outp, inp, weight, alpha):\n",
    "    # calculate the error in output, this on its own goes for bias correction\n",
    "    del_out = error * der_act_func(outp)\n",
    "    # mulitply it by the input, this comes from the partial derivate wrt the weights\n",
    "    del_w = np.dot(inp, del_out.T)\n",
    "    # calculate error that we're going to propagate to the previous layer\n",
    "    prev_err = np.dot(weight, del_out )\n",
    "    return weight + alpha * del_w, prev_err, del_out # update\n",
    "\n",
    "# initialize the biases\n",
    "def init_bias(num_neur_per_lay, nonzeros=False):\n",
    "    b_list = []\n",
    "    np.random.seed(1)\n",
    "    for i in range(len(num_neur_per_lay)-1):\n",
    "    # initialize the weights so that they're in the right format\n",
    "        if nonzeros:\n",
    "            w = 2 * np.random.random((num_neur_per_lay[i+1],1)) - 1\n",
    "            b_list.append(w)\n",
    "        else:\n",
    "            w = 2 * np.zeros((num_neur_per_lay[i+1],1)) - 1\n",
    "            b_list.append(w)     \n",
    "    return b_list\n",
    "\n",
    "# softmax for the output layer (not yet used)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load input and label__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "(2, 4)\n",
      "(784, 36850)\n",
      "(10, 36850)\n",
      "(28038, 784)\n",
      "(28038, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import NeuralNets\n",
    "\n",
    "x = np.array([[0,0,1],[1,1,1],[1,0,1],[0,1,1]]).T\n",
    "y = np.array([[0,0],[1,0],[1,0],[0,0]]).T\n",
    "print(np.shape(x))\n",
    "print(np.shape(y))\n",
    "\n",
    "X = mnist.train.images#[0:10000]\n",
    "Y = mnist.train.labels#[0:10000]\n",
    "bool_0 = (Y == np.array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])).all(1)\n",
    "bool_1 = (Y == np.array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])).all(1)\n",
    "bool_2 = (Y == np.array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])).all(1)\n",
    "bool_3 = (Y == np.array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])).all(1)\n",
    "bool_4 = (Y == np.array([[ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])).all(1)\n",
    "bool_5 = (Y == np.array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])).all(1)\n",
    "bool_6 = (Y == np.array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]])).all(1)\n",
    "bool_7 = (Y == np.array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]])).all(1)\n",
    "bool_8 = (Y == np.array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]])).all(1)\n",
    "bool_9 = (Y == np.array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])).all(1)\n",
    "bool_ = (bool_0 | bool_1 | bool_2 | bool_3 | bool_4)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X[bool_], Y[bool_], test_size=0.33, random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "###### SUPER important to have them in the right format/dimension\n",
    "x_train = x_train.T\n",
    "x_test=x_test.T\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T\n",
    "# print(x_train)\n",
    "# print(y_train)\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(y_train))\n",
    "# print(X.T)\n",
    "# print(Y.T)\n",
    "print(np.shape(X[bool_]))\n",
    "print(np.shape(Y[bool_]))\n",
    "# print(len(mnist.train.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSf0/k3EU2BPqPLMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11974a6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# just plot one\n",
    "import matplotlib.pyplot as plt\n",
    "# print(x_train.T[0])\n",
    "print(np.shape(x_train.T[0].reshape(28,28)))\n",
    "plt.imshow(x_train.T[0].reshape(28,28), cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initialize weights and biases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Number of nodes per layers. First layer is the input, last is the output \n",
      "[784 300  10]\n"
     ]
    }
   ],
   "source": [
    "num_inputs = int(x_train.shape[0])\n",
    "num_outputs = y_train.shape[0]\n",
    "print(num_outputs)\n",
    "###### modify this if you want to have more layer or change the #of nodes per layer\n",
    "num_neur_per_lay = np.array([num_inputs,300,num_outputs])\n",
    "## e.g. num_neur_per_lay = np.array([num_inputs,300,100,num_outputs]), \n",
    "## gives you 2 hidden layer with 300 nodes in the first one and 100 in the second one\n",
    "\n",
    "print 'Number of nodes per layers. First layer is the input, last is the output '\n",
    "print num_neur_per_lay\n",
    "num_lay = len(num_neur_per_lay)-1\n",
    "\n",
    "# initialize weights based on input and output format\n",
    "w_list = init_weights(num_neur_per_lay)\n",
    "# initialize biases based on input and output format\n",
    "b = init_bias(num_neur_per_lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO-DO work on this one to do the log-likelihood instead\n",
    "def error_func(output, y_label, is_squared=True):\n",
    "    if is_squared:\n",
    "        error = y_label - output \n",
    "    else:\n",
    "        error = np.divide(y_label, output)\n",
    "    return error\n",
    "\n",
    "# log-likelihood (also called cross-entropy\n",
    "# error function)\n",
    "# y*log(o)\n",
    "# y/o*do/dw\n",
    "# y/o*o*(1-o)*dz/dw\n",
    "# y*(1-o)*dz/dw\n",
    "\n",
    "# sum squared error function)\n",
    "# 1/2*(y-o)^2\n",
    "# -(y-o)*do/dw\n",
    "# -(y-o)*o*(1-o)*dz/dw\n",
    "# (y-o)*o*(1-o)*dz/dw\n",
    "# out_err = error_func(y_train, o, True)\n",
    "# print(out_err)\n",
    "# out_err = error_func(o, y_train, True)\n",
    "# test_out_err = (o_test, y_test, True)\n",
    "# print(np.shape(y_train - o ))\n",
    "# print(out_err.shape)\n",
    "# ((y_train - o ) == out_err).all()\n",
    "# print(test_out_err.shape)\n",
    "# print(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Before\n",
      "[784 300  10]\n",
      "Training error = 0.440704364288\n",
      "Training accuracy = 0.118181818182\n",
      "After 0 out of 1000 backprop. Training error = 0.36523388178, test error = 0.365240030546\n",
      "Training accuracy = 0.118398914518, test accuracy = 0.119008264463\n",
      "After 5 out of 1000 backprop. Training error = 0.177449785092, test error = 0.17699893846\n",
      "Training accuracy = 0.12803256445, test accuracy = 0.126666666667\n",
      "After 10 out of 1000 backprop. Training error = 0.135586485039, test error = 0.135106195353\n",
      "Training accuracy = 0.13986431479, test accuracy = 0.137741046832\n",
      "After 15 out of 1000 backprop. Training error = 0.123725221447, test error = 0.123259729061\n",
      "Training accuracy = 0.150122116689, test accuracy = 0.146776859504\n",
      "After 20 out of 1000 backprop. Training error = 0.118145116648, test error = 0.117713099542\n",
      "Training accuracy = 0.158344640434, test accuracy = 0.154435261708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-73bcaf0ec145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# For the hidden layers, compute the error and correction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_lay\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mnew_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# update weight for hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mw_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ea0e99c086c7>\u001b[0m in \u001b[0;36mupdate_w\u001b[0;34m(error, outp, inp, weight, alpha)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mdel_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# calculate error that we're going to propagate to the previous layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mprev_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_out\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdel_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_out\u001b[0m \u001b[0;31m# update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # # initialize weights and biases if you want to restart\n",
    "# w_list = init_weights(num_neur_per_lay)\n",
    "# #bias\n",
    "# b = init_bias(num_neur_per_lay )\n",
    "print('Before')\n",
    "print(num_neur_per_lay)\n",
    "# print(w_list)\n",
    "# print(b)\n",
    "[np.shape(w_) for w_ in w_list]\n",
    "\n",
    "alpha = 0.00001\n",
    "\n",
    "N_epoch = 1000\n",
    "\n",
    "# Feed example through network to compute output with random weights\n",
    "o, h_list = neur_net(x_train, w_list, b, num_lay)\n",
    "\n",
    "# For the output layer, compute the error and correction\n",
    "out_err = (y_train-o)\n",
    "\n",
    "print 'Training error = '+str(np.mean(np.abs(out_err)))\n",
    "print 'Training accuracy = '+str(sum(np.argmax(o,0) == np.argmax(y_train,0)) / float(np.shape(o)[1]))\n",
    "\n",
    "for i in range(N_epoch):\n",
    "\n",
    "    new_weight, prev_err, del_b = update_w(out_err, h_list[-1], h_list[-2], w_list[-1], alpha)\n",
    "    # update weight for output layer\n",
    "    w_list[-1] = new_weight\n",
    "    # update bias for output layer\n",
    "    b[-1] = b[-1] + alpha * np.mean(del_b,1,keepdims=True)\n",
    "    \n",
    "    # For the hidden layers, compute the error and correction\n",
    "    for j in range(num_lay-1):\n",
    "        new_weight, prev_err, del_b = update_w(prev_err, h_list[-2-j], h_list[-3-j], w_list[-2-j], alpha)\n",
    "        # update weight for hidden layer\n",
    "        w_list[-2-j] = new_weight\n",
    "        # update bias for hidden layers\n",
    "        b[-2-j] = b[-2-j] + alpha * np.mean(del_b,1,keepdims=True)\n",
    "        \n",
    "    # Feed example through network to compute output with updated weights\n",
    "    o, h_list = neur_net(x_train, w_list, b, num_lay)\n",
    "    \n",
    "    # For the output layer, compute the error and correction with updated weights\n",
    "    out_err = error_func(o, y_train, True)\n",
    "#     out_err = (y_train - o)\n",
    "    \n",
    "    \n",
    "    if (i% 5) == 0:\n",
    "        # neural net on test set\n",
    "        o_test, h_list_test = neur_net(x_test, w_list, b, num_lay)\n",
    "\n",
    "        # test error\n",
    "        test_out_err = error_func(o_test, y_test, True)\n",
    "        \n",
    "        print 'After '+ str(i)+' out of ' + str(N_epoch) + ' backprop. Training error = '+str(np.mean(np.abs(out_err)))+\\\n",
    "            ', test error = ' + str(np.mean(np.abs(test_out_err)))\n",
    "        print 'Training accuracy = '+str(sum(np.argmax(o,0) == np.argmax(y_train,0)) / float(np.shape(o)[1]))+\\\n",
    "            ', test accuracy = ' + str(sum(np.argmax(o_test,0) == np.argmax(y_test,0)) / float(np.shape(o_test)[1]))\n",
    "\n",
    "# print(w_list)\n",
    "# print(b)\n",
    "# print(x)\n",
    "# [np.shape(w_) for w_ in w_list]\n",
    "# print(o_test[:,3])\n",
    "# print(np.argmax(o_test[:,3],0))\n",
    "# print(y_test[:,3])\n",
    "# print(np.argmax(y_test[:,3],0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I've run 1000 nodes and it reached 75% accuracy, but again, this is with squared error, not log-likelihood yet. That should be better for sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Not needed__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new output [1, 0, 0]\n",
      "Result[[ 0.99986946]\n",
      " [ 0.17011654]]\n"
     ]
    }
   ],
   "source": [
    "print 'new output [1, 0, 0]'\n",
    "o, h_list = neur_net(np.array([[1,0,0]]).T, w_list, b, num_lay)\n",
    "print 'Result' + str(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00922196  0.00777619  0.00653753 -0.0077548 ]\n",
      " [-0.00541096 -0.00078916 -0.00237167 -0.00180415]]\n",
      "[[ 0.00922196  0.00777619  0.00653753  0.0077548 ]\n",
      " [ 0.00541096  0.00078916  0.00237167  0.00180415]]\n",
      "0.00520830384613\n"
     ]
    }
   ],
   "source": [
    "out_err = (y-o)\n",
    "print(out_err)\n",
    "# np.mean(\n",
    "print(np.abs(out_err))\n",
    "print(np.mean(np.abs(out_err)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000001"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.mean(np.abs(test_out_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test==o_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(o_test[:,:3],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18150,)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.argmax(o_test,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(o_test,0) == np.argmax(y_test,0)) / np.shape(o_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(o_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 5, 5, ..., 2, 4, 9])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.84744453  2.84744453  2.84744453  2.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]\n",
      " [ 3.84744453  3.84744453  3.84744453  3.84744453]]\n",
      "[0 0 0 0]\n",
      "[[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[2 2 0 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(o_test[:,:4])\n",
    "print(-np.log(softmax(o_test[:,:4])))\n",
    "print(np.argmax(softmax(o_test[:,:4]),0))\n",
    "print(y_test[:,:4])\n",
    "print(np.argmax(y_test[:,:4],0))\n",
    "np.argmax(o_test[:,:4],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
